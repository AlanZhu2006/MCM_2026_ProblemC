\begin{thebibliography}{99}

\bibitem{lightgbm}
Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., ... \& Liu, T. Y. (2017). 
LightGBM: A highly efficient gradient boosting decision tree. 
\textit{Advances in neural information processing systems}, 30.

\bibitem{scipy}
Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M., Reddy, T., Cournapeau, D., ... \& van Mulbregt, P. (2020). 
SciPy 1.0: fundamental algorithms for scientific computing in Python. 
\textit{Nature methods}, 17(3), 261-272.

\bibitem{sklearn}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... \& Duchesnay, E. (2011). 
Scikit-learn: Machine learning in Python. 
\textit{Journal of machine learning research}, 12(Oct), 2825-2830.

\bibitem{pandas}
McKinney, W. (2010). 
Data structures for statistical computing in python. 
\textit{Proceedings of the 9th Python in Science Conference}, 445, 51-56.

\bibitem{numpy}
Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., ... \& Oliphant, T. E. (2020). 
Array programming with NumPy. 
\textit{Nature}, 585(7825), 357-362.

\bibitem{timeseries_cv}
Bergmeir, C., \& Benítez, J. M. (2012). 
On the use of cross-validation for time series predictor evaluation. 
\textit{Information Sciences}, 191, 192-213.

\bibitem{feature_importance}
Breiman, L. (2001). 
Random forests. 
\textit{Machine learning}, 45(1), 5-32.

\bibitem{monte_carlo}
Metropolis, N., \& Ulam, S. (1949). 
The Monte Carlo method. 
\textit{Journal of the American statistical association}, 44(247), 335-341.

\bibitem{optimization}
Kraft, D. (1988). 
A software package for sequential quadratic programming. 
\textit{DFVLR-FB 88-28, DLR German Aerospace Center – Institute for Flight Mechanics, Koln, Germany}.

\bibitem{differential_evolution}
Storn, R., \& Price, K. (1997). 
Differential evolution–a simple and efficient heuristic for global optimization over continuous spaces. 
\textit{Journal of global optimization}, 11(4), 341-359.

\bibitem{dwts_data}
COMAP. (2026). 
2026 MCM Problem C: Data With The Stars. 
\textit{Mathematical Contest in Modeling}.

\bibitem{inverse_problem}
Tikhonov, A. N., \& Arsenin, V. Y. (1977). 
\textit{Solutions of ill-posed problems}. 
V. H. Winston \& Sons.

\bibitem{constrained_optimization}
Nocedal, J., \& Wright, S. (2006). 
\textit{Numerical optimization}. 
Springer Science \& Business Media.

\bibitem{ensemble_methods}
Zhou, Z. H. (2012). 
\textit{Ensemble methods: foundations and algorithms}. 
CRC press.

\bibitem{gradient_boosting}
Friedman, J. H. (2001). 
Greedy function approximation: a gradient boosting machine. 
\textit{Annals of statistics}, 1189-1232.

\bibitem{cross_validation}
Kohavi, R. (1995). 
A study of cross-validation and bootstrap for accuracy estimation and model selection. 
\textit{Proceedings of the 14th international joint conference on Artificial intelligence}, 2, 1137-1143.

\bibitem{feature_selection}
Guyon, I., \& Elisseeff, A. (2003). 
An introduction to variable and feature selection. 
\textit{Journal of machine learning research}, 3(Mar), 1157-1182.

\bibitem{time_series_forecasting}
Hyndman, R. J., \& Athanasopoulos, G. (2018). 
\textit{Forecasting: principles and practice}. 
OTexts.

\bibitem{statistical_comprehensive statistical analysis}
Montgomery, D. C., Peck, E. A., \& Vining, G. G. (2021). 
\textit{Introduction to linear regression comprehensive statistical analysis}. 
John Wiley \& Sons.

\bibitem{uncertainty_quantification}
Smith, R. C. (2013). 
\textit{Uncertainty quantification: theory, implementation, and applications}. 
SIAM.

\bibitem{kaggle_dwts}
Brupley. (n.d.). 
Dancing with the Stars Dataset. 
\textit{Kaggle}. 
Retrieved from \url{https://www.kaggle.com/datasets/brupley/dancing-with-the-stars-dataset}

\bibitem{github_dwts_analysis}
Aidendodd. (n.d.). 
Dancing-with-the-Stars-Data-Analysis. 
\textit{GitHub}. 
Retrieved from \url{https://github.com/Aidendodd/Dancing-with-the-Stars-Data-Analysis}

\end{thebibliography}
